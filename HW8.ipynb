{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–¥–∞–Ω–∏–µ 1\n",
    "–ù–∞–π–¥–∏—Ç–µ —Å–∞–º—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–≤–∏—Ç—ã –∞–≤—Ç–æ—Ä–æ–≤, author_name –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ—Å—Ç–æ–∏—Ç –∏–∑, –∫–∞–∫ –º–∏–Ω–∏–º—É–º, –¥–≤—É—Ö —Å–ª–æ–≤. –í—ã–≤–µ–¥–∏—Ç–µ —á–µ—Ä–µ–∑ –ø—Ä–æ–±–µ–ª: author_name –∏ —Ç–≤–∏—Ç.\n",
    "hint: min(x, key=len)\n",
    "\n",
    "**p.s. –ü–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å—Ç–∞—Ä–∞–π—Ç–µ—Å—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –º–æ–∂–Ω–æ –º–µ–Ω—å—à–µ —Ü–∏–∫–ª–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–≥–∏–∫—É –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>created_at</th>\n",
       "      <th>handle</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>len_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Matt Woods</td>\n",
       "      <td>2019-07-07 14:13:18</td>\n",
       "      <td>matopher</td>\n",
       "      <td>6396</td>\n",
       "      <td>1560</td>\n",
       "      <td>Everyone should find 3 hobbies:\\n\\nOne that makes you rich. üí∞ \\nOne that keeps you fit. üèãÔ∏è‚Äç‚ôÇÔ∏è\\nOne that makes you smart. üìñ\\n\\nh/t: @nava</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Thought Reps</td>\n",
       "      <td>2019-07-07 13:18:34</td>\n",
       "      <td>ThoughtReps</td>\n",
       "      <td>333</td>\n",
       "      <td>59</td>\n",
       "      <td>‚ÄúYou‚Äôre not going to get rich renting out your time.‚Äù \\n\\nOne of the many brilliant pieces of wisdom from @naval on the @joerogan podcast. \\n\\nCheck out more:\\n</td>\n",
       "      <td>155.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Naval Ravikant Bot</td>\n",
       "      <td>2019-07-06 20:00:35</td>\n",
       "      <td>NavalBot</td>\n",
       "      <td>3171</td>\n",
       "      <td>683</td>\n",
       "      <td>\"The fundamental delusion - there is something out there that will make me happy and fulfilled forever.\" - @nava</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Naval Ravikant Bot</td>\n",
       "      <td>2019-07-05 20:00:54</td>\n",
       "      <td>NavalBot</td>\n",
       "      <td>1950</td>\n",
       "      <td>411</td>\n",
       "      <td>\"Unnecessary meetings (and most are) are a mutually-assured-destruction of time. Learning how to avoid them is a prerequisite of doing anything great.\" - @nava</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Shivam Bhardwaj</td>\n",
       "      <td>2019-07-05 17:57:40</td>\n",
       "      <td>sbhardwaj7529</td>\n",
       "      <td>2140</td>\n",
       "      <td>215</td>\n",
       "      <td>The @naval podcast was such a goldmine that I decided to print the entire transcript in the form of a book.\\nAbsolutely lovin it. :)</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           author_name           created_at         handle  likes  retweets  \\\n",
       "20          Matt Woods  2019-07-07 14:13:18       matopher   6396      1560   \n",
       "22        Thought Reps  2019-07-07 13:18:34    ThoughtReps    333        59   \n",
       "25  Naval Ravikant Bot  2019-07-06 20:00:35       NavalBot   3171       683   \n",
       "27  Naval Ravikant Bot  2019-07-05 20:00:54       NavalBot   1950       411   \n",
       "28     Shivam Bhardwaj  2019-07-05 17:57:40  sbhardwaj7529   2140       215   \n",
       "\n",
       "                                                                                                                                                       tweet_content  \\\n",
       "20                          Everyone should find 3 hobbies:\\n\\nOne that makes you rich. üí∞ \\nOne that keeps you fit. üèãÔ∏è‚Äç‚ôÇÔ∏è\\nOne that makes you smart. üìñ\\n\\nh/t: @nava   \n",
       "22  ‚ÄúYou‚Äôre not going to get rich renting out your time.‚Äù \\n\\nOne of the many brilliant pieces of wisdom from @naval on the @joerogan podcast. \\n\\nCheck out more:\\n   \n",
       "25                                                  \"The fundamental delusion - there is something out there that will make me happy and fulfilled forever.\" - @nava   \n",
       "27   \"Unnecessary meetings (and most are) are a mutually-assured-destruction of time. Learning how to avoid them is a prerequisite of doing anything great.\" - @nava   \n",
       "28                             The @naval podcast was such a goldmine that I decided to print the entire transcript in the form of a book.\\nAbsolutely lovin it. :)    \n",
       "\n",
       "    len_tweet  \n",
       "20      130.0  \n",
       "22      155.0  \n",
       "25      112.0  \n",
       "27      159.0  \n",
       "28      132.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets1 = tweets.copy()\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "# –∏–∑–±–∞–≤–ª—è–µ–º—Å—è –æ—Ç –∏–º–µ–Ω, –≤ –∫–æ—Ç–æ—Ä—ã—Ö —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ —Å–ª–æ–≤–æ\n",
    "tweets1['author_name'] = tweets1['author_name'].apply(lambda x: str(x).split() )\n",
    "tweets1 = tweets1[tweets1['author_name'].str.len() >= 2]\n",
    "tweets1['author_name'] = tweets1['author_name'].str.join(' ')\n",
    "# –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç–æ–±–µ—Ü —Å –¥–ª–∏–Ω–Ω–æ–π —Ç–≤–∏—Ç–∞ (–º–æ–∂–Ω–æ –∏ –±–µ–∑ –Ω–µ–≥–æ, –Ω–æ —Ö–æ—Ç–µ–ª–æ—Å—å –≤–∏–¥–µ—Ç—å –¥–ª–∏–Ω—É) –¥–ª–∏–Ω–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º, –∞ –Ω–µ —Å–ª–æ–≤–∞–º\n",
    "tweets1['len_tweet'] = tweets1['tweet_content'].str.len()\n",
    "tweets1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>len_tweet</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>author_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">modest proposal</th>\n",
       "      <th>5369</th>\n",
       "      <td>ü§î</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5409</th>\n",
       "      <td>üî•üî•</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5363</th>\n",
       "      <td>üëç</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mark Ames</th>\n",
       "      <th>8850</th>\n",
       "      <td>ü§î</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>modest proposal</th>\n",
       "      <th>5328</th>\n",
       "      <td>ü§î</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Julian Togelius</th>\n",
       "      <th>14068</th>\n",
       "      <td>Simple statistical methods are shown to much better than fancy machine learning on a whole bunch of real-world sequence-prediction datasets. The reason: the time series used are tiny by ML standards, and all the ML methods overfit. &lt;Q&gt; The https://t.co/S3BpRgtxUW paper with its finding that the worse Stat forecasting method was more accurate than the best of the ML ones has passed the 100,000 mark of views/downloads. None of those who have read/downloaded it has challenged its finding. We are still waiting!  &lt;/Q&gt;</td>\n",
       "      <td>518.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>John Durant</th>\n",
       "      <th>8440</th>\n",
       "      <td>My god, Ed Buck wasn‚Äôt ‚Äúa small-time Democratic donor‚Äù and it wasn‚Äôt just a pattern of exchanging drugs and money for sexual favors.\\n\\nEd Buck had a sexual fetish for forcing young black men to take meth, getting them hooked on it, and raping them ‚Äî killing two. &lt;Q&gt; Two men died in the home of Ed Buck, a small-time Democratic donor and political activist. Now federal prosecutors say they have detailed a disturbing pattern in which he exchanged drugs and money for sexual favors, citing at least 11 victims. https://t.co/5d1utUPzPF  &lt;/Q&gt;</td>\n",
       "      <td>539.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rachael Fiddis üéÆ</th>\n",
       "      <th>8383</th>\n",
       "      <td>It's easy to get angry at this but, I feel sorry for him, not in a condescending way, more imagine what he has missed out on due to an uneducated opinion. \\nThe incredible gaming community, making random, bad-ass friends, the mind-blowing visual experiences - the list is endless! &lt;Q&gt; Video games are the absolute worst Loser Habit you could have \\n\\nHours &amp;amp; hours spent on an ultimately useless skill \\n\\nSitting and getting fatter and weaker and skinnier and paler \\n\\nCompletely impaired social skills \\n\\nVideo games make you a bottom tier subhuma &lt;/Q&gt;</td>\n",
       "      <td>551.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KEEM üçø</th>\n",
       "      <th>8497</th>\n",
       "      <td>I became a self made millionaire &amp;amp; met some of the greatest people on the planet due to video games.  Video games have saved inter city kids from the gangs &amp;amp; helped those with disability‚Äôs to have a social life. It‚Äôs also helping our military &amp;amp; civilian pilots. You are uneducated &lt;Q&gt; Video games are the absolute worst Loser Habit you could have \\n\\nHours &amp;amp; hours spent on an ultimately useless skill \\n\\nSitting and getting fatter and weaker and skinnier and paler \\n\\nCompletely impaired social skills \\n\\nVideo games make you a bottom tier subhuma &lt;/Q&gt;</td>\n",
       "      <td>564.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nina Teicholz</th>\n",
       "      <th>8250</th>\n",
       "      <td>I get these unbelievable #keto stories on my twitter feed every day. Yet hardly ever do people send before/after photos like this using the Mediterranean diet, a low-fat diet, or vegetarian diet. I'm open to those stories! Just not seeing them (which is not science, I know) &lt;Q&gt; Had a rough weekend mentally so I‚Äôm reminding myself that 2.5yrs ago I wore a 7XL shirt &amp;amp; today an XL. Definitely reminds me the hard work is worth it! #nsv #mindset #keto #ketodiet #ketogenic #lowcarb #lchf #nsng #weightlossjourney #weightloss #weightlosstransformation #fitness  &lt;/Q&gt;</td>\n",
       "      <td>568.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2214 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       tweet_content  \\\n",
       "author_name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "modest proposal  5369                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ü§î    \n",
       "                 5409                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             üî•üî•   \n",
       "                 5363                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             üëç    \n",
       "Mark Ames        8850                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ü§î    \n",
       "modest proposal  5328                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             ü§î    \n",
       "...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ...   \n",
       "Julian Togelius  14068                                                        Simple statistical methods are shown to much better than fancy machine learning on a whole bunch of real-world sequence-prediction datasets. The reason: the time series used are tiny by ML standards, and all the ML methods overfit. <Q> The https://t.co/S3BpRgtxUW paper with its finding that the worse Stat forecasting method was more accurate than the best of the ML ones has passed the 100,000 mark of views/downloads. None of those who have read/downloaded it has challenged its finding. We are still waiting!  </Q>   \n",
       "John Durant      8440                                  My god, Ed Buck wasn‚Äôt ‚Äúa small-time Democratic donor‚Äù and it wasn‚Äôt just a pattern of exchanging drugs and money for sexual favors.\\n\\nEd Buck had a sexual fetish for forcing young black men to take meth, getting them hooked on it, and raping them ‚Äî killing two. <Q> Two men died in the home of Ed Buck, a small-time Democratic donor and political activist. Now federal prosecutors say they have detailed a disturbing pattern in which he exchanged drugs and money for sexual favors, citing at least 11 victims. https://t.co/5d1utUPzPF  </Q>   \n",
       "Rachael Fiddis üéÆ 8383               It's easy to get angry at this but, I feel sorry for him, not in a condescending way, more imagine what he has missed out on due to an uneducated opinion. \\nThe incredible gaming community, making random, bad-ass friends, the mind-blowing visual experiences - the list is endless! <Q> Video games are the absolute worst Loser Habit you could have \\n\\nHours &amp; hours spent on an ultimately useless skill \\n\\nSitting and getting fatter and weaker and skinnier and paler \\n\\nCompletely impaired social skills \\n\\nVideo games make you a bottom tier subhuma </Q>   \n",
       "KEEM üçø           8497   I became a self made millionaire &amp; met some of the greatest people on the planet due to video games.  Video games have saved inter city kids from the gangs &amp; helped those with disability‚Äôs to have a social life. It‚Äôs also helping our military &amp; civilian pilots. You are uneducated <Q> Video games are the absolute worst Loser Habit you could have \\n\\nHours &amp; hours spent on an ultimately useless skill \\n\\nSitting and getting fatter and weaker and skinnier and paler \\n\\nCompletely impaired social skills \\n\\nVideo games make you a bottom tier subhuma </Q>   \n",
       "Nina Teicholz    8250       I get these unbelievable #keto stories on my twitter feed every day. Yet hardly ever do people send before/after photos like this using the Mediterranean diet, a low-fat diet, or vegetarian diet. I'm open to those stories! Just not seeing them (which is not science, I know) <Q> Had a rough weekend mentally so I‚Äôm reminding myself that 2.5yrs ago I wore a 7XL shirt &amp; today an XL. Definitely reminds me the hard work is worth it! #nsv #mindset #keto #ketodiet #ketogenic #lowcarb #lchf #nsng #weightlossjourney #weightloss #weightlosstransformation #fitness  </Q>   \n",
       "\n",
       "                        len_tweet  \n",
       "author_name                        \n",
       "modest proposal  5369         2.0  \n",
       "                 5409         2.0  \n",
       "                 5363         2.0  \n",
       "Mark Ames        8850         2.0  \n",
       "modest proposal  5328         2.0  \n",
       "...                           ...  \n",
       "Julian Togelius  14068      518.0  \n",
       "John Durant      8440       539.0  \n",
       "Rachael Fiddis üéÆ 8383       551.0  \n",
       "KEEM üçø           8497       564.0  \n",
       "Nina Teicholz    8250       568.0  \n",
       "\n",
       "[2214 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = tweets1.groupby(by=['author_name']).apply(lambda x: x[x['len_tweet'] == x['len_tweet'].min()])[['tweet_content', 'len_tweet']]\n",
    "y.sort_values(by='len_tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1['tweet_content'] = tweets1['tweet_content'].astype(str) \n",
    "x = tweets1.groupby('author_name').apply(lambda x: min(x['tweet_content'], key=len))\n",
    "# print(x.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–¥–∞–Ω–∏–µ 2\n",
    " –í—ã–≤–µ–¥–∏—Ç–µ —á–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —ç–º–æ–¥–∑–∏, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤ —Ç–≤–∏—Ç–∞—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('‚úÖ', 117), ('üëá', 101), ('üòÇ', 101), ('üî•', 52), ('üèª', 41), ('üôè', 41), ('üèΩ', 38), ('üëè', 34), ('‚ù§', 32), ('‚ôÇ', 30), ('üö®', 30), ('üá∏', 30), ('üá∫', 29), ('üèº', 26), ('üë®', 20), ('‚ö´', 19), ('üë©', 18), ('üëç', 18), ('üíØ', 17), ('üèæ', 17), ('ü§î', 17), ('üôå', 16), ('üÜï', 16), ('‚ôÄ', 15), ('üöÄ', 15)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import emoji\n",
    "\n",
    "all_tweets = tweets.to_string(columns=['tweet_content'])\n",
    "emo_count = Counter()\n",
    "for char in all_tweets:\n",
    "    if char in emoji.UNICODE_EMOJI:\n",
    "        emo_count[char] += 1 \n",
    "        \n",
    "print(emo_count.most_common(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ó–∞–¥–∞–Ω–∏–µ 3\n",
    "–ò—Å–ø–æ–ª—å–∑—É—è —Ñ—É–Ω–∫—Ü–∏—é apply, –ø–æ—Å—á–∏—Ç–∞–π—Ç–µ —Å—É–º–º—É sentiment score –≤—Å–µ—Ö —Ç–≤–∏—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è —Å–ª–æ–≤–∞—Ä–∏: https://github.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/tree/master/data/opinion-lexicon-English\n",
    "\n",
    "    1 –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–ª–æ–≤\n",
    "    -1 –¥–ª—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ WordNetLemmatizer\n",
    "\n",
    "–†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–ø–∏—à–∏—Ç–µ –≤ –Ω–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É sentiment_score\n",
    "\n",
    "\n",
    "\n",
    "task 3 edit:\n",
    "\n",
    "–ü–æ—Å–ª–µ —Ç–æ–≥–æ, –∫–∞–∫ –ø–æ—Å—á–∏—Ç–∞–µ—Ç–µ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç —Ç–≤–∏—Ç—Ç–æ–≤, –æ—Ç—Å–æ—Ä—Ç–∏—Ä—É–π—Ç–µ –≤—Å–µ —Ç–≤–∏—Ç—Ç—ã –∏:\n",
    "1. –°–æ–∑–¥–∞–π—Ç–µ —Ç–∞–±–ª–∏—Ü—É —Å–∞–º—ã—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–≤–∏—Ç—Ç–æ–≤ (–ø–æ –∫–∞–∫–æ–º—É-–Ω–∏–±—É–¥—å –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—é) \n",
    "2. –í—ã–≤–µ–¥–∏—Ç–µ —Ç–æ–ø 10 –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–æ —Å–µ–Ω—Ç–∏–º–µ–Ω—Ç—É —Ç–≤–∏—Ç—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>tweet_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naval</td>\n",
       "      <td>Unresolved thoughts, prematurely pushed out of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naval</td>\n",
       "      <td>The modern mind is overstimulated and the mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naval</td>\n",
       "      <td>The Lindy Effect for startups:\\n\\nThe longer y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naval</td>\n",
       "      <td>@orangebook_ This was a good tweet.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naval</td>\n",
       "      <td>Social media lowers the cost of raising &amp;amp; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_name                                      tweet_content\n",
       "0       Naval  Unresolved thoughts, prematurely pushed out of...\n",
       "1       Naval  The modern mind is overstimulated and the mode...\n",
       "2       Naval  The Lindy Effect for startups:\\n\\nThe longer y...\n",
       "3       Naval               @orangebook_ This was a good tweet. \n",
       "4       Naval  Social media lowers the cost of raising &amp; ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "tweets3 = tweets.copy()\n",
    "tweets3.drop(columns=['created_at', 'handle', 'likes', 'retweets'], inplace=True)\n",
    "tweets3.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i go and buy two nice bunny\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(text):\n",
    "\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    text = text.split()\n",
    "    \n",
    "    # –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è —Å —á—Ä-—Ç—ç–≥–∞–º–∏, —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–º–Ω–æ–≥–æ –¥–æ–ª—å—à–µ\n",
    "    tag_map = defaultdict(lambda : wordnet.NOUN)\n",
    "    tag_map['J'] = wordnet.ADJ\n",
    "    tag_map['V'] = wordnet.VERB\n",
    "    tag_map['R'] = wordnet.ADV\n",
    "    res = [wn.lemmatize(x, tag_map[tag[0]]) for x, tag in pos_tag(text)]\n",
    "    \n",
    "    # –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è –±–µ–∑ —á—Ä-—Ç—ç–≥–æ–≤\n",
    "    # res = [wn.lemmatize(x) for x in text]\n",
    "    \n",
    "    return ' '.join(res)\n",
    "print(lemmatize('I went and bought two nice bunnies!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naval</td>\n",
       "      <td>Unresolved thoughts, prematurely pushed out of...</td>\n",
       "      <td>unresolved thought prematurely push out of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naval</td>\n",
       "      <td>The modern mind is overstimulated and the mode...</td>\n",
       "      <td>the modern mind be overstimulated and the mode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naval</td>\n",
       "      <td>The Lindy Effect for startups:\\n\\nThe longer y...</td>\n",
       "      <td>the lindy effect for startup the longer you go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naval</td>\n",
       "      <td>@orangebook_ This was a good tweet.</td>\n",
       "      <td>orangebook_ this be a good tweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naval</td>\n",
       "      <td>Social media lowers the cost of raising &amp;amp; ...</td>\n",
       "      <td>social medium lower the cost of raise amp join...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_name                                      tweet_content  \\\n",
       "0       Naval  Unresolved thoughts, prematurely pushed out of...   \n",
       "1       Naval  The modern mind is overstimulated and the mode...   \n",
       "2       Naval  The Lindy Effect for startups:\\n\\nThe longer y...   \n",
       "3       Naval               @orangebook_ This was a good tweet.    \n",
       "4       Naval  Social media lowers the cost of raising &amp; ...   \n",
       "\n",
       "                                              lemmas  \n",
       "0  unresolved thought prematurely push out of the...  \n",
       "1  the modern mind be overstimulated and the mode...  \n",
       "2  the lindy effect for startup the longer you go...  \n",
       "3                   orangebook_ this be a good tweet  \n",
       "4  social medium lower the cost of raise amp join...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets3['lemmas'] = tweets3['tweet_content'].apply(lemmatize)\n",
    "tweets3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('negative-words.txt','r') as file:\n",
    "    negative  = file.read()\n",
    "    negative = negative.split(';;;\\n')[2].split()\n",
    "with open('positive-words.txt','r') as file:\n",
    "    positive  = file.read()\n",
    "    positive = positive.split(';;;\\n')[2].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modern society will shame you for earn money shame you for be happy shame you for be raise well shame you for have child and ultimately shame you for exist it isnt just religion that control you by declare you a sinner\n",
      "-2\n"
     ]
    }
   ],
   "source": [
    "def sentiments(lemmas):\n",
    "    total = 0\n",
    "    for x in lemmas.split():\n",
    "        if x in positive:\n",
    "            total += 1\n",
    "        if x in negative:\n",
    "            total -= 1\n",
    "    return total\n",
    "\n",
    "my_text = tweets3['lemmas'][80]\n",
    "print(my_text)\n",
    "print(sentiments(my_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naval</td>\n",
       "      <td>Unresolved thoughts, prematurely pushed out of...</td>\n",
       "      <td>unresolved thought prematurely push out of the...</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naval</td>\n",
       "      <td>The modern mind is overstimulated and the mode...</td>\n",
       "      <td>the modern mind be overstimulated and the mode...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naval</td>\n",
       "      <td>The Lindy Effect for startups:\\n\\nThe longer y...</td>\n",
       "      <td>the lindy effect for startup the longer you go...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naval</td>\n",
       "      <td>@orangebook_ This was a good tweet.</td>\n",
       "      <td>orangebook_ this be a good tweet</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Naval</td>\n",
       "      <td>Social media lowers the cost of raising &amp;amp; ...</td>\n",
       "      <td>social medium lower the cost of raise amp join...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  author_name                                      tweet_content  \\\n",
       "0       Naval  Unresolved thoughts, prematurely pushed out of...   \n",
       "1       Naval  The modern mind is overstimulated and the mode...   \n",
       "2       Naval  The Lindy Effect for startups:\\n\\nThe longer y...   \n",
       "3       Naval               @orangebook_ This was a good tweet.    \n",
       "4       Naval  Social media lowers the cost of raising &amp; ...   \n",
       "\n",
       "                                              lemmas  sentiment_score  \n",
       "0  unresolved thought prematurely push out of the...               -3  \n",
       "1  the modern mind be overstimulated and the mode...                3  \n",
       "2  the lindy effect for startup the longer you go...                0  \n",
       "3                   orangebook_ this be a good tweet                1  \n",
       "4  social medium lower the cost of raise amp join...               -1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets3['sentiment_score'] = tweets3['lemmas'].apply(sentiments)\n",
    "tweets3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\asus\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_name</th>\n",
       "      <th>tweet_content</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Orange Book</td>\n",
       "      <td>Growth only requires the courage to understand...</td>\n",
       "      <td>growth only require the courage to understand ...</td>\n",
       "      <td>-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sahil Lavingia</td>\n",
       "      <td>Being a founder is hard. Being an early employ...</td>\n",
       "      <td>be a founder be hard be an early employee be h...</td>\n",
       "      <td>-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wealth Theory ‚Ñ¢</td>\n",
       "      <td>When a naive investor who saw no risk suddenly...</td>\n",
       "      <td>when a naive investor who saw no risk suddenly...</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Krishna</td>\n",
       "      <td>Unease, anxiety, tension, stress, worry - all ...</td>\n",
       "      <td>unease anxiety tension stress worry all form o...</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wealth Theory ‚Ñ¢</td>\n",
       "      <td>When a naive investor who saw no risk suddenly...</td>\n",
       "      <td>when a naive investor who saw no risk suddenly...</td>\n",
       "      <td>-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4639</th>\n",
       "      <td>Mr. Plenty</td>\n",
       "      <td>Women liking rich men doesn‚Äôt make them gold d...</td>\n",
       "      <td>woman like rich men doesnt make them gold digg...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4640</th>\n",
       "      <td>Alexander J.A Cortes</td>\n",
       "      <td>Tactical Virtues of Masculinity (ala Jack Dono...</td>\n",
       "      <td>tactical virtue of masculinity ala jack donova...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4641</th>\n",
       "      <td>The Ancient Sage</td>\n",
       "      <td>Being kind to others is the most effective way...</td>\n",
       "      <td>be kind to others be the most effective way to...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4642</th>\n",
       "      <td>The Ancient Sage</td>\n",
       "      <td>Spiritual awakening is the only real freedom.\\...</td>\n",
       "      <td>spiritual awakening be the only real freedom f...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4643</th>\n",
       "      <td>Zen Black</td>\n",
       "      <td>Truth lies in a bit of this and a bit of that....</td>\n",
       "      <td>truth lie in a bit of this and a bit of that y...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4644 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               author_name                                      tweet_content  \\\n",
       "0              Orange Book  Growth only requires the courage to understand...   \n",
       "1           Sahil Lavingia  Being a founder is hard. Being an early employ...   \n",
       "2          Wealth Theory ‚Ñ¢  When a naive investor who saw no risk suddenly...   \n",
       "3                  Krishna  Unease, anxiety, tension, stress, worry - all ...   \n",
       "4          Wealth Theory ‚Ñ¢  When a naive investor who saw no risk suddenly...   \n",
       "...                    ...                                                ...   \n",
       "4639            Mr. Plenty  Women liking rich men doesn‚Äôt make them gold d...   \n",
       "4640  Alexander J.A Cortes  Tactical Virtues of Masculinity (ala Jack Dono...   \n",
       "4641      The Ancient Sage  Being kind to others is the most effective way...   \n",
       "4642      The Ancient Sage  Spiritual awakening is the only real freedom.\\...   \n",
       "4643             Zen Black  Truth lies in a bit of this and a bit of that....   \n",
       "\n",
       "                                                 lemmas  sentiment_score  \n",
       "0     growth only require the courage to understand ...              -13  \n",
       "1     be a founder be hard be an early employee be h...              -12  \n",
       "2     when a naive investor who saw no risk suddenly...              -11  \n",
       "3     unease anxiety tension stress worry all form o...              -11  \n",
       "4     when a naive investor who saw no risk suddenly...              -11  \n",
       "...                                                 ...              ...  \n",
       "4639  woman like rich men doesnt make them gold digg...               10  \n",
       "4640  tactical virtue of masculinity ala jack donova...               10  \n",
       "4641  be kind to others be the most effective way to...               10  \n",
       "4642  spiritual awakening be the only real freedom f...               10  \n",
       "4643  truth lie in a bit of this and a bit of that y...               14  \n",
       "\n",
       "[4644 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd —Å 5 –ø—Ä–æ—Ü —Å–∞–º—ã—Ö –ø–æ–ª–æ–∂–∏—Ç–µ–ª–Ω—ã—Ö –∏ —Å–∞–º—ã—Ö –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö —Ç–≤–∏—Ç–æ–≤\n",
    "percent = tweets3['sentiment_score'].quantile([.05, .95]).values\n",
    "print(percent[0], percent[1])\n",
    "neg = tweets3[(tweets3['sentiment_score'] <= percent[0] ) | (tweets3['sentiment_score'] >= percent[1])]\n",
    "neg.sort_values(by='sentiment_score', inplace=True)\n",
    "neg.reset_index(drop=True, inplace=True)\n",
    "neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_neg = Counter(negative)\n",
    "all_pos = Counter(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_tweets = tweets3[tweets3['sentiment_score'] > 0].to_string(columns=['lemmas'])\n",
    "neg_tweets = tweets3[tweets3['sentiment_score'] < 0].to_string(columns=['lemmas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_c = Counter(pos_tweets.split())\n",
    "neg_c = Counter(neg_tweets.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('good', 1555),\n",
       " ('like', 1450),\n",
       " ('work', 1233),\n",
       " ('great', 825),\n",
       " ('best', 688),\n",
       " ('right', 654),\n",
       " ('love', 621),\n",
       " ('well', 512),\n",
       " ('free', 410),\n",
       " ('enough', 362)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = pos_c.keys() & all_pos.keys()\n",
    "res = Counter(list(keys))\n",
    "for key in keys:\n",
    "    res[key] += pos_c[key]\n",
    "    \n",
    "res.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('problem', 602),\n",
       " ('bad', 531),\n",
       " ('lose', 402),\n",
       " ('hard', 394),\n",
       " ('fear', 389),\n",
       " ('wrong', 286),\n",
       " ('lie', 222),\n",
       " ('waste', 213),\n",
       " ('risk', 210),\n",
       " ('lack', 209)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys2 = neg_c.keys() & all_neg.keys()\n",
    "res2 = Counter(list(keys2))\n",
    "for key in keys2:\n",
    "    res2[key] += neg_c[key]\n",
    "    \n",
    "res2.most_common(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
